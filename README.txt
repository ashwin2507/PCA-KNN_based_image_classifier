{\rtf1\ansi\ansicpg1252\cocoartf1671
{\fonttbl\f0\fswiss\fcharset0 Helvetica-Bold;\f1\fswiss\fcharset0 Helvetica;\f2\froman\fcharset0 TimesNewRomanPSMT;
}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid1\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\margl1440\margr1440\vieww38200\viewh20640\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\b\fs24 \cf0 Image classifier using PCA and KNN
\f1\b0 \
\
In this project I have used four different types of fruits, you can increase the types of fruits by adding images to the training and testing data. For image classification the steps taken are 1) resizing the training and testing images 2) feature elimination using PCA  3) predicting the class label using K-Nearest Neighbors. The dataset is downloaded from Kaggle.\
Learning outcomes:\
1) Understanding the importance of PCA by implementing the algorithm from scratch.\
2) Feature elimination using the scree plot and visualizing the transformed data.\
3) KNN algorithm by implementing the algorithm from scratch.\
\

\f0\b For in-depth explanation please refer to the project report.
\f1\b0 \
\
Programming Language Used: Python 3.6, Inc.\
--------------------------\
\
----------------\
CODE STRUCTURE\
-----------------\
\pard\tx916\tx1832\tx2748\tx3664\tx4580\tx5496\tx6412\tx7328\tx8244\tx9160\tx10076\tx10992\tx11908\tx12824\tx13740\tx14656\pardeftab720\ri0\partightenfactor0
\ls1\ilvl0
\f2 \cf0 1) 	Loading the Training Data and extracting the labels of each:Used open CV (cv2) to load the image data and extract the labels of each image, resized the image to 28*28. \
\pard\pardeftab720\ri0\partightenfactor0
\ls1\ilvl0\cf0 2)	Implementing PCA as a function: First Flatten the image data to 1899(no. of training samples) by (28*28*3), then standardize the data, find co-variance matrix, find eigen-value, eigen-vector and lastly plotted the proportion of variance graph to decide how many Principal components to pick, in this case I picked 2 principal components, transformed the data to 2-D using the eigen-vector and eigen-value. \
3) 	Load the Testing data, resize to 28* 28 and perform PCA on it as well. The code looks like this.\
4) 	Find K-nearest data points (using Euclidean distance) and take a majority vote to predict the label.\
\pard\tx720\pardeftab720\ri0\partightenfactor0
\cf0 \
\pard\tx720\pardeftab720\ri0\partightenfactor0

\f1 \cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
--------------------\
HOW TO RUN THE CODE\
--------------------\
Please ensure you have these packages:\
import numpy as np\
import cv2\
import matplotlib.pyplot as plt\
from matplotlib.offsetbox import OffsetImage, AnnotationBbox\
import glob\
import os\
from sklearn.decomposition import PCA # To check if the output of using .pca() matches my implementaion\
from sklearn.preprocessing import StandardScaler # For using sklearn.pca()\
import math\
import operator\
\
Data set is also in the folder copy and paste it to your machine and change the path of the dataset in the code according to your local computer path stored in.\
\
Command\
-------\
python3 image_classifier.py\
\
The program will output graphs and plots and lastly will prompt the user for entering the value of k desired for KNN.\
}